---
permalink: /
title: "Robin Armingaud"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a research engineer at CEA List, working on large language models (LLMs) for manufacturing applications. My research focuses on adapting LLMs for domain-specific tasks with an emphasis on efficiency, as well as exploring few-shot and zero-shot learning for information extraction. I enjoy tackling real-world problems with LLMs, and I regularly participate in Kaggle competitions and conference workshop.

## News

- **[2025]** *GLiDRE*, our new document-level relation extraction model for **zero-shot** relation extraction, will be released soon! Inspired by the excellent [GLiNER](https://github.com/urchade/GLiNER), it's designed for low-data scenarios.

- **[2025]** We participated again in the **EvaLLM 2025** workshop. Our participation paper will be available shortly!

- **[2025]** My article on the **TextMine 2025** competition is now online! Read it here: [GLiDRE](https://hal.science/hal-04918406v1/file/GLIDRE_version_longue%20%282%29.pdf)

- **[2025]** The first (buggy!) version of *GLiDRE* ranked **10th** in the [TextMine 2025 Kaggle competition](https://www.kaggle.com/c/defi-text-mine-2025/leaderboard). Promising results.

- **[2025]** Participation in the [Drawing with LLMs](https://www.kaggle.com/competitions/drawing-with-llms) Kaggle competition. My solution involved fine-tuning a **Mistral-7B** model using **synthetic data** generated with **Gemini**.

- **[2025]** Participation in the [AI Mathematical Olympiad â€“ Progress Prize 2](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2) Kaggle competition. We explored different prompt strategies using **DeepSeek R1**.

- **[2024]** We successfully adapted a **RoBERTa** model for the **manufacturing domain** using curated internet data and advanced data selection strategies, achieving **state-of-the-art** results on internal benchmarks.

- **[2024]** We won **1st place** at the **EvaLLM 2024** challenge, which required extracting entities from just **four training examples**. Our winning approach combined **GLiNER**, **synthetic data generation with a Mistral model**, and **majority voting** for robust performance. Read it here: [CEA-List@EvalLLM2024](https://hal.science/hal-04678063v1/document)

- **[2024]** Participation in the [LLM Prompt Recovery](https://www.kaggle.com/competitions/llm-prompt-recovery) Kaggle competition. Our solution combined **DeBERTa classifiers** for template filling, a **fine-tuned T5** model trained on synthetic data, and a **prompted Mistral-7B** model to generate candidate prompts. A final **judge model** selected the best candidate.
